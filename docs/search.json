[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist, writer, educator & podcaster. My interests include promoting data & AI literacy/fluency, helping to spread data skills through organizations and society and lowering the barrier to entry for data science, analysis, and machine learning. I’m currently Head of Developer Relations at Outerbounds, a company committed to building infrastructure that provides a solid foundation for machine learning and AI applications of all shapes and sizes. I am also the host of the industry podcast Vanishing Gradients. Find out more here."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html",
    "href": "posts/fine-tuning-llms-gpt-2/index.html",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "",
    "text": "In a recent episode of the Vanishing Gradients podcast, Sebastian Raschka, an AI researcher and educator, dove into the fascinating world of large language models (LLMs) and their applications. During the podcast, Sebastian shared valuable insights on the LLM lifecycle, fine-tuning techniques, and practical considerations for working with these powerful models.\nOne of the highlights of the episode was a live coding session, where Sebastian demonstrated how to fine-tune a small local GPT model for a specific task: spam classification. Fine-tuning pre-trained LLMs for specific tasks is a crucial skill in the field of natural language processing (NLP), as it allows us to adapt these models to solve real-world problems efficiently.\nIn this blog post, we’ll take a closer look at Sebastian’s live coding session and break down the steps involved in fine-tuning a local GPT model for spam classification. We’ll explore the following topics:\n\nDataset and preprocessing techniques\nLoading pre-trained weights from OpenAI\nModel architecture modifications\nTraining process and evaluation metrics\nInference and results\nPotential improvements and further experiments\n\nBy the end of this post, you’ll have a solid understanding of how to fine-tune LLMs for your own projects and a practical example to guide you along the way.\nYou can watch it below and you can run all the code in Lightning Studio here!\n\n\n\n\nNote: we used Claude Opus to help write this post, based on the podcast transcript. You can also check out all the greatest hits of this podcast episode here."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html#introduction",
    "href": "posts/fine-tuning-llms-gpt-2/index.html#introduction",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "",
    "text": "In a recent episode of the Vanishing Gradients podcast, Sebastian Raschka, an AI researcher and educator, dove into the fascinating world of large language models (LLMs) and their applications. During the podcast, Sebastian shared valuable insights on the LLM lifecycle, fine-tuning techniques, and practical considerations for working with these powerful models.\nOne of the highlights of the episode was a live coding session, where Sebastian demonstrated how to fine-tune a small local GPT model for a specific task: spam classification. Fine-tuning pre-trained LLMs for specific tasks is a crucial skill in the field of natural language processing (NLP), as it allows us to adapt these models to solve real-world problems efficiently.\nIn this blog post, we’ll take a closer look at Sebastian’s live coding session and break down the steps involved in fine-tuning a local GPT model for spam classification. We’ll explore the following topics:\n\nDataset and preprocessing techniques\nLoading pre-trained weights from OpenAI\nModel architecture modifications\nTraining process and evaluation metrics\nInference and results\nPotential improvements and further experiments\n\nBy the end of this post, you’ll have a solid understanding of how to fine-tune LLMs for your own projects and a practical example to guide you along the way.\nYou can watch it below and you can run all the code in Lightning Studio here!\n\n\n\n\nNote: we used Claude Opus to help write this post, based on the podcast transcript. You can also check out all the greatest hits of this podcast episode here."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html#dataset-and-preprocessing",
    "href": "posts/fine-tuning-llms-gpt-2/index.html#dataset-and-preprocessing",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "Dataset and Preprocessing",
    "text": "Dataset and Preprocessing\nTo get started, Sebastian used a dataset consisting of text messages labeled as either “spam” or “ham” (non-spam). To keep the training fast (30 seconds on GPU, 10 minutes on laptop), he used a relatively small dataset and relatively small GPT-2 model.\nTo preprocess the data, Sebastian followed these steps:\n\nLoaded the dataset using pandas:\n\nPandas is a popular library for data manipulation and analysis in Python\nIt provides a convenient way to load datasets from various file formats, such as CSV or Excel\nIf needed, polars can be used, which is another library that can be used for similar purposes, offering some performance advantages in certain scenarios\n\nEncoded the labels as integers (0 for “ham” and 1 for “spam”)\n\nConverting the text labels to integers is a common practice in machine learning\nIt allows the model to work with numerical data, which is more efficient and easier to process\nIn this case, “ham” messages are assigned the label 0, while “spam” messages are assigned the label 1\n\nSplit the data into train, validation, and test sets\n\nSplitting the dataset into separate subsets is crucial for evaluating the model’s performance and preventing overfitting\nThe training set is used to update the model’s parameters during the learning process\nThe validation set is used to tune hyperparameters and monitor the model’s performance during training\nThe test set is used to assess the model’s final performance on unseen data and provide an unbiased estimate of its generalization ability\n\n\nSince the dataset was not balanced, Sebastian balanced it himself to ensure that the accuracy metric is meaningful (avoiding the need for F1 score, precision, and recall). Imbalanced datasets can lead to misleading accuracy scores, as a model that simply predicts the majority class can achieve high accuracy without actually learning to discriminate between the classes. By balancing the dataset, Sebastian ensured that the model’s performance is not biased towards the majority class and that the accuracy metric provides a fair assessment of its classification abilities. (In a real-world application, we may work with larger, imbalanced dataset; but a balanced dataset is used for simplicity here.)\nAfter preprocessing the data, Sebastian created PyTorch datasets and data loaders to efficiently batch and shuffle the data during training. PyTorch datasets provide a convenient way to encapsulate the data and apply transformations, while data loaders handle batching, shuffling, and parallelization. He aimed for the following number of batches:\n\nTraining batches: 130\nValidation batches: 19\nTest batches: 30\n\nThese data loaders were used to feed the data into the model during the training and evaluation phases, just like in scikit-learn when training classifiers like logistic regression, naive Bayes, or SVMs. This abstraction allows for a consistent and efficient way of handling data across different machine learning frameworks and libraries."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html#loading-pre-trained-weights-and-modifying-the-architecture",
    "href": "posts/fine-tuning-llms-gpt-2/index.html#loading-pre-trained-weights-and-modifying-the-architecture",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "Loading Pre-trained Weights and Modifying the Architecture",
    "text": "Loading Pre-trained Weights and Modifying the Architecture\n\nSebastian loaded a pre-trained GPT-2 model using weights from OpenAI into his own reimplementation of GPT. He started with the smallest GPT-2 model but kept in mind that larger models can be used as well. The different model sizes have the same architecture, with the same transformer blocks and attention heads, but differ in the number of blocks and heads. This allows for flexibility in choosing the model size based on the available computational resources and the complexity of the task at hand.\nBefore fine-tuning, Sebastian checked if the loaded model works by generating some text. This is an important step to ensure that the model has been loaded correctly and is functioning as expected. By generating text, he verified that the model is able to produce coherent and meaningful outputs based on the given prompt.\nNext, Sebastian examined the model architecture, which consists of:\n\nToken embeddings\nPositional embeddings\nOptional dropout\nRepeated transformer blocks with multi-head attention\n\nUnderstanding the architecture of the pre-trained model is crucial for making informed decisions about how to modify it for the specific task.\nTo adapt GPT-2 for the spam classification task, Sebastian modified the model’s architecture by replacing the final layer with a linear layer that serves as a binary classifier head. The original GPT-2 model is designed for language modeling and text generation, so its final layer outputs a probability distribution over the entire vocabulary. However, for the spam classification task, we only need a binary output indicating whether a message is spam or not. By replacing the final layer with a binary classifier head, Sebastian adapted the model to the specific requirements of the task.\nHe also froze all layers except the classifier head and the last transformer block. Freezing the layers means that their parameters will not be updated during the fine-tuning process. This is a common technique used to prevent overfitting and reduce the computational cost of fine-tuning. By freezing the pre-trained layers, Sebastian leveraged the language understanding capabilities already learned by the model and focused on adapting the final layers to the specific task. The last transformer block was left unfrozen to allow for some task-specific adjustments to the model’s representations."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html#training-and-evaluation",
    "href": "posts/fine-tuning-llms-gpt-2/index.html#training-and-evaluation",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "Training and Evaluation",
    "text": "Training and Evaluation\nWith the modified GPT-2 architecture in place, Sebastian set up the training loop using PyTorch. He defined the optimizer and loss function (cross-entropy loss) and iterated over the training data loader, performing forward and backward passes and updating the model parameters.\nThe optimizer is responsible for updating the model’s parameters based on the computed gradients. In this case, Sebastian used the Adam optimizer, which is a popular choice for training deep learning models. The cross-entropy loss function measures the discrepancy between the predicted probabilities and the true labels, providing a signal for the model to learn from.\nDuring each training iteration, Sebastian:\n\nPerformed a forward pass to compute the model’s predictions\nCalculated the loss\nPerformed a backward pass to compute the gradients\nUpdated the model’s parameters using the optimizer\n\n\nSebastian defined an evaluation function to compute the loss and accuracy over the validation set. Evaluating the model on the validation set during training allows for monitoring the model’s performance on unseen data and helps in detecting overfitting. If the validation loss starts increasing while the training loss continues to decrease, it’s an indication that the model is starting to overfit to the training data.\nBefore training the model, Sebastian evaluated the baseline performance of the model, which achieved around 50% accuracy before fine-tuning as expected, which is equivalent to random guessing in a binary classification task.\nNext, Sebastian ran the training for 5 epochs and monitored the progress. An epoch represents a complete pass through the entire training datasetDuring the live coding session, Sebastian encountered a gotcha: forgetting to use the no_grad context or inference mode. In PyTorch, when you perform operations on tensors, it builds a computation graph that keeps track of the gradients. This is necessary for training the model, but during evaluation or inference, we don’t need to compute gradients. Forgetting to use no_grad or inference mode can lead to unnecessary memory usage and slower performance. Sebastian fixed this issue and continued training."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html#inference-and-results",
    "href": "posts/fine-tuning-llms-gpt-2/index.html#inference-and-results",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "Inference and Results",
    "text": "Inference and Results\nAfter the first round of training, Sebastian evaluated the fine-tuned model on the test set. He initially achieved an accuracy of around 80%, which is a significant improvement over the random baseline of 50%. Then, after fixing another gotcha and rerunning the notebook,the model achieved an accuracy of 95% on the test set. This high accuracy indicates that the fine-tuned GPT-2 model has successfully learned to distinguish between spam and ham messages with a high degree of accuracy.\nTo further demonstrate the effectiveness of fine-tuning, Sebastian compared the fine-tuned model’s performance to the pre-trained GPT-2 model. The pre-trained model, without any fine-tuning, is not specifically adapted to the spam classification task and may not perform as well. By comparing the results, he highlighted the importance of fine-tuning for achieving high performance on specific tasks.\nSebastian also experimented with a larger GPT-2 model and observed that it generates less repetitive text and performs even better. This suggests that using larger models with more capacity can lead to improved performance, especially for more complex tasks.\nThe iterative process of fine-tuning, evaluating, and making adjustments is a common practice in machine learning. By analyzing the model’s performance and identifying areas for improvement, practitioners can gradually refine the model and achieve better results. The jump from 50% accuracy to 95% accuracy in Sebastian’s live coding session demonstrates the power of this iterative approach and the potential for significant performance gains through fine-tuning."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html#potential-improvements-and-further-experiments",
    "href": "posts/fine-tuning-llms-gpt-2/index.html#potential-improvements-and-further-experiments",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "Potential Improvements and Further Experiments",
    "text": "Potential Improvements and Further Experiments\nTo further optimize the model’s performance and generalize the fine-tuning approach to other tasks and datasets, Sebastian explored the following potential improvements and experiments:\n\nTrying different learning rates and optimizers\n\nThe choice of learning rate and optimizer can have a significant impact on the model’s convergence speed and final performance. Experimenting with different learning rates (e.g., higher or lower values) and alternative optimizers (e.g., SGD, RMSprop) can help find the optimal configuration for the specific task.\n\nAdjusting the number of frozen layers\n\nWhile freezing the majority of the layers helps prevent overfitting and reduces computational cost, it may also limit the model’s ability to adapt to the specific task. Experimenting with unfreezing more layers or using a gradual unfreezing scheme can allow for more fine-grained adaptation while still benefiting from the pre-trained weights.\n\nExperimenting with other pre-trained LLMs (e.g., BERT, RoBERTa)\n\nGPT-2 is just one of many powerful pre-trained language models available. Trying out other models, such as BERT (Bidirectional Encoder Representations from Transformers) or RoBERTa (Robustly Optimized BERT Pretraining Approach), can provide different insights and potentially lead to better performance on certain tasks.\n\nFine-tuning the model on larger datasets\n\nThe size and quality of the dataset play a crucial role in the model’s performance. When available, fine-tuning the model on larger datasets can help the model learn more robust and generalizable representations. However, it’s important to ensure that the larger dataset is still relevant to the task at hand and properly labeled.\n\nApplying the fine-tuning approach to other text classification tasks\n\nThe fine-tuning approach demonstrated in this live coding session can be applied to a wide range of text classification tasks, such as sentiment analysis, topic classification, or intent recognition. Experimenting with different datasets and tasks can validate the effectiveness of the approach and expand its applicability.\n\n\nBy exploring these variations and iteratively improving the fine-tuning process, practitioners can unlock the full potential of pre-trained language models and develop highly accurate and efficient solutions for a wide range of NLP problems."
  },
  {
    "objectID": "posts/fine-tuning-llms-gpt-2/index.html#conclusion",
    "href": "posts/fine-tuning-llms-gpt-2/index.html#conclusion",
    "title": "Fine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka",
    "section": "Conclusion",
    "text": "Conclusion\nSebastian’s live coding session provides a hands-on demonstration of fine-tuning GPT-2 for spam classification. The key takeaways from this session include:\n\nFine-tuning pre-trained LLMs is a powerful and cost-effective technique for adapting them to specific tasks.\nThe process involves modifying the model architecture, freezing layers, and training on task-specific data.\nEvaluation is crucial for monitoring the model’s performance and making improvements.\nFine-tuned LLMs can achieve high accuracy on specific tasks, such as spam classification.\nExperimenting with different model sizes and architectures can lead to better performance.\nIterative fine-tuning, evaluation, and adjustments can result in significant performance gains, as demonstrated by the improvement from 50% to 95% accuracy in Sebastian’s session.\n\nBy following the steps outlined in this blog post and experimenting with different techniques, practitioners can apply the power of fine-tuned LLMs to their own projects and unlock new possibilities in natural language processing.\nIf you enjoyed this, you can follow Sebastian on twitter here and Hugo here. You can also subscribe to Hugo’s fortnightly Vanishing Gradients Newsletter here."
  },
  {
    "objectID": "posts/42-Lessons-in-AI/index.html",
    "href": "posts/42-Lessons-in-AI/index.html",
    "title": "42 Lessons from a Year of Building with AI Systems",
    "section": "",
    "text": "I recently did a 3 hour livestream for Vanishing Gradients with Eugene Yan (Amazon), Bryan Bischof (Hex), Charles Frye (Modal), Hamel Husain (Parlance Labs), and Shreya Shankar (UC Berkeley).\n​Over the past year, these five guests have been building real-world applications on top of LLMs. They have identified crucial and often neglected lessons that are essential for developing and building AI products.\nThey have recently written an O’Reilly report (also published here) based on these learnings and, in this conversation, they shared advice and lessons for anyone who wants to build products informed by LLMs, ranging from tactical to operational and strategic.\nWe’ve just now released two podcast episodes of this conversation (also on Spotify etc…):\nYou can also watch the livestream here on YouTube:\nEven if you’re not specifically working on/with LLMs, there’s a serious amount of general data science, machine learning, and AI wisdom packed into these 3 hours.\nLet us know what you get out of it on Twitter (@hugobowne and @vanishingdata) or LinkedIn. You can also register for future livestreams onour lu.ma calendar and subscribe to our YouTube channel.\nHere are some clips that may be of interest!"
  },
  {
    "objectID": "posts/42-Lessons-in-AI/index.html#pro-tips-for-building-with-llms",
    "href": "posts/42-Lessons-in-AI/index.html#pro-tips-for-building-with-llms",
    "title": "42 Lessons from a Year of Building with AI Systems",
    "section": "Pro tips for building with LLMs",
    "text": "Pro tips for building with LLMs\n\n\n🥷Pro tips for building with LLMs:🧠 @sh_reya : Challenge the RAG vs fine-tuning discourse📊 @BEBischof: Optimize prompts for relevance, boosts performance🚫 @charles_irl: 'Just because you can cache a million context tokens doesn't mean you should'😱 @BEBischof: Reality… pic.twitter.com/AGgWrr7hKn\n\n— Hugo Bowne-Anderson (@hugobowne) June 24, 2024"
  },
  {
    "objectID": "posts/42-Lessons-in-AI/index.html#no-structured-approach-youre-not-really-doing-ai.",
    "href": "posts/42-Lessons-in-AI/index.html#no-structured-approach-youre-not-really-doing-ai.",
    "title": "42 Lessons from a Year of Building with AI Systems",
    "section": "No structured approach? You’re not really doing AI.",
    "text": "No structured approach? You’re not really doing AI.\n\n\nI like the way @HamelHusain puts it https://t.co/41zi5El3JS\n\n— Simon Willison (@simonw) June 23, 2024"
  },
  {
    "objectID": "posts/42-Lessons-in-AI/index.html#the-ai-engineer-data-literacy-divide",
    "href": "posts/42-Lessons-in-AI/index.html#the-ai-engineer-data-literacy-divide",
    "title": "42 Lessons from a Year of Building with AI Systems",
    "section": "The AI Engineer Data Literacy Divide",
    "text": "The AI Engineer Data Literacy Divide\n\n\nIn most cases i’ve encountered in the wild, the title “AI Engineer” is harmful. I explain why in the below video https://t.co/cJT89x2fO6\n\n— Hamel Husain (@HamelHusain) June 21, 2024"
  },
  {
    "objectID": "posts/42-Lessons-in-AI/index.html#build-durable-systems-not-just-chasing-models",
    "href": "posts/42-Lessons-in-AI/index.html#build-durable-systems-not-just-chasing-models",
    "title": "42 Lessons from a Year of Building with AI Systems",
    "section": "Build durable systems, not just chasing models",
    "text": "Build durable systems, not just chasing models\n\n\n🚀 @eugeneyan's top 3 AI lessons (asked by @shreya):1️⃣ Continuous evals throughout dev 2️⃣ BM25 for retrieval - a hidden gem 💎 3️⃣ Build durable systems, not just chasing modelsGame-changing advice from their report 🔥 #AIEngineering (this hashtag is for @HamelHusain)… pic.twitter.com/hPpVCgKCDG\n\n— Hugo Bowne-Anderson (@hugobowne) June 21, 2024"
  },
  {
    "objectID": "posts/42-Lessons-in-AI/index.html#what-we-covered",
    "href": "posts/42-Lessons-in-AI/index.html#what-we-covered",
    "title": "42 Lessons from a Year of Building with AI Systems",
    "section": "What We Covered",
    "text": "What We Covered\n\n\n🎙️High level points we covered today in \"Lessons from a Year of Building with LLMs\" with @eugeneyan @BEBischof @sh_reya @HamelHusain @charles_irl @jxnlcoWatch here: https://t.co/G3KaYjkB0R@vanishingdata podcast out next week 💫 pic.twitter.com/WjgAPIzxIg\n\n— Hugo Bowne-Anderson (@hugobowne) June 21, 2024\n\n\nCheck it out and let us know what you get out of it on Twitter (@hugobowne and @vanishingdata) or LinkedIn. You can also register for future livestreams onour lu.ma calendar and subscribe to our YouTube channel."
  },
  {
    "objectID": "posts/local-llms/index.html",
    "href": "posts/local-llms/index.html",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "",
    "text": "I recently hosted a session with Simon Willison for Hamel Husain and Dan Becker’s Mastering LLMs: A Conference For Developers & Data Scientists. Simon’s talk, which I encourage you to watch, was all about\nYou can watch it here and read more about it here:\nWhat I actually want to write about is spurred by Dan Becker asking me why I like using local LLMs and I don’t think I had a satisfactory answer at the time. To explore this topic, I chatted with Claude and ChatGPT, which generated the following list of points based on our conversations:"
  },
  {
    "objectID": "posts/local-llms/index.html#data-privacy-and-security",
    "href": "posts/local-llms/index.html#data-privacy-and-security",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "1. Data Privacy and Security",
    "text": "1. Data Privacy and Security\n\nControl Over Data: Running models locally ensures that sensitive data never leaves your machine, providing higher levels of privacy and security.\nCompliance: For industries with strict data regulations, local processing helps in adhering to compliance requirements."
  },
  {
    "objectID": "posts/local-llms/index.html#performance-and-latency",
    "href": "posts/local-llms/index.html#performance-and-latency",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "2. Performance and Latency",
    "text": "2. Performance and Latency\n\nReduced Latency: Local models eliminate the need for network requests, resulting in faster response times, especially important for real-time applications.\nConsistency: Local processing avoids variability in latency that can occur due to network issues or server-side processing delays."
  },
  {
    "objectID": "posts/local-llms/index.html#cost-efficiency",
    "href": "posts/local-llms/index.html#cost-efficiency",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "3. Cost Efficiency",
    "text": "3. Cost Efficiency\n\nLower Costs: Avoiding cloud-based service fees can be cost-effective, especially for frequent or large-scale usage.\nPredictable Expenses: Costs are more predictable as they mainly involve hardware and occasional software updates rather than ongoing cloud service charges."
  },
  {
    "objectID": "posts/local-llms/index.html#customization-and-control",
    "href": "posts/local-llms/index.html#customization-and-control",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "4. Customization and Control",
    "text": "4. Customization and Control\n\nTailored Solutions: Local deployment allows for greater customization of the models and integration with specific workflows.\nFull Control: Users have full control over the environment, configurations, and updates, which is beneficial for debugging and optimizing performance."
  },
  {
    "objectID": "posts/local-llms/index.html#offline-capabilities",
    "href": "posts/local-llms/index.html#offline-capabilities",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "5. Offline Capabilities",
    "text": "5. Offline Capabilities\n\nOffline Access: Local models can function without an internet connection, which is crucial for remote locations or scenarios with unreliable connectivity.\nDisaster Recovery: Ensures that applications remain operational even during network outages or cloud service disruptions."
  },
  {
    "objectID": "posts/local-llms/index.html#learning-and-experimentation",
    "href": "posts/local-llms/index.html#learning-and-experimentation",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "6. Learning and Experimentation",
    "text": "6. Learning and Experimentation\n\nHands-On Experience: Running models locally provides a deeper understanding of how they work, beneficial for learning and research.\nExperimentation: Facilitates rapid experimentation and prototyping without the constraints or delays of cloud-based services."
  },
  {
    "objectID": "posts/local-llms/index.html#open-source-and-community-support",
    "href": "posts/local-llms/index.html#open-source-and-community-support",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "7. Open Source and Community Support",
    "text": "7. Open Source and Community Support\n\nCommunity Innovation: Many local deployment tools are open source, fostering community contributions and rapid innovation.\nTransparency: Open source tools provide transparency in how models are implemented and executed, which can build trust and facilitate customization."
  },
  {
    "objectID": "posts/local-llms/index.html#scalability-for-development",
    "href": "posts/local-llms/index.html#scalability-for-development",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "8. Scalability for Development",
    "text": "8. Scalability for Development\n\nDevelopment and Testing: Local environments are ideal for development and testing before deploying to production environments, ensuring robustness and reliability.\nResource Optimization: Allows for efficient use of local resources, scaling up as needed without relying on cloud infrastructure."
  },
  {
    "objectID": "posts/local-llms/index.html#ethical-and-environmental-considerations",
    "href": "posts/local-llms/index.html#ethical-and-environmental-considerations",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "9. Ethical and Environmental Considerations",
    "text": "9. Ethical and Environmental Considerations\n\nSustainable Practices: Using local resources can be more environmentally friendly by reducing the energy consumption associated with cloud data centers.\nEthical Considerations: Promotes ethical use of AI by keeping sensitive data within trusted boundaries and reducing dependency on centralized services."
  },
  {
    "objectID": "posts/local-llms/index.html#autonomy-and-independence",
    "href": "posts/local-llms/index.html#autonomy-and-independence",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "10. Autonomy and Independence",
    "text": "10. Autonomy and Independence\n\nIndependence from Providers: Reduces dependency on cloud service providers, giving users more autonomy and reducing the risk of vendor lock-in.\nResilience: Enhances resilience against changes in cloud service policies, pricing, or availability.\n\nDo any of these resonate with you? I’d be excited to hear which ones on twitter and/or LinkedIn."
  },
  {
    "objectID": "posts/local-llms/index.html#getting-started-with-local-llms",
    "href": "posts/local-llms/index.html#getting-started-with-local-llms",
    "title": "10 Brief Arguments for Local LLMs and AI",
    "section": "Getting Started with Local LLMs",
    "text": "Getting Started with Local LLMs\n\nOllama is a great way to get started locally with SOTA models, such as Llama 3, Phi 3, Mistral, and Gemma;\nSimon’s LLM cli utility allows you to explore LLMs of all kinds from the command and has all the fun mentioned above: can be piped according to unix-like philosophy, logs to local sqlite db, can be explored interactively using datasette, can work with embeddings to build RAG apps, and more!\nLlamaFile is a great project from Mozilla and Justine Tunney that is “an open source initiative that collapses all the complexity of a full-stack LLM chatbot down to a single file that runs on six operating systems”, has a cool front-end GUI, and you can get up and running with lots of models, including multimodal models such as LLaVa, immediately;\nLM Studio is one of the more advanced GUIs I’ve seen to interact with local LLMs: you can discover new models on the homepage, browse and download lots of models from HuggingFace, easily chat with models, and even chat with several simultaneously to compare responses, latency, and more.\nOobagooba’s text generation webUI, which allows you to interact with local models (and others) through a webUI – lots of fun stuff for fine-tuning, chatting and so on.\n\nI use all of these tools a bunch and hope to demo some of them soon and perhaps do some write-ups so let me know on twitter and/or LinkedIn, if you’d find this useful, and I’ll likely get to it sooner!"
  },
  {
    "objectID": "posts/pierre-menard/index.html",
    "href": "posts/pierre-menard/index.html",
    "title": "ChatGPT, Author of The Quixote",
    "section": "",
    "text": "In the era of generative AI, copyright won’t be enough. In fact, it’s the wrong place to look.\nTl;dr\nIn Borges’ fable Pierre Menard, Author of The Quixote, the eponymous Monsieur Menard plans to sit down and write a portion of Cervantes’ Don Quixote. Not to transcribe, but re-write the epic novel word for word:\nHe first tried to do so by becoming Cervantes, learning Spanish, and forgetting all the history since Cervantes wrote Don Quixote, among other things, but then decided it would make more sense to (re)write the text as Menard himself. The narrator tells us that “the Cervantes text and the Menard text are verbally identical, but the second is almost infinitely richer.” Perhaps this is an inversion of the ability of Generative AI models (LLMs, text-to-image, and more) to reproduce swathes of their training data without those chunks being explicitly stored in the model and its weights: the output is verbally identical to the original but reproduced probabilistically without any of the human blood, sweat, tears, and life experience that goes into the creation of human writing and cultural production."
  },
  {
    "objectID": "posts/pierre-menard/index.html#generative-ai-has-a-plagiarism-problem",
    "href": "posts/pierre-menard/index.html#generative-ai-has-a-plagiarism-problem",
    "title": "ChatGPT, Author of The Quixote",
    "section": "Generative AI Has a Plagiarism Problem",
    "text": "Generative AI Has a Plagiarism Problem\nChatGPT, for example, doesn’t memorize its training data, per se. As Mike Loukides and Tim O’Reilly astutely point out,\n\nA model prompted to write like Shakespeare may start with the word “To,” which makes it slightly more probable that it will follow that with “be,” which makes it slightly more probable that the next word will be “or” – and so forth.\n\nSo then, as it turns out, next-word prediction (and all the sauce on top) can reproduce chunks of training data. This is the basis of the NYTimes lawsuit against OpenAI. I have been able to convince ChatGPT to give me large chunks of novels that are in the public domain, such as those on Project Gutenberg, including Pride and Prejudice. Researchers are finding more and more ways to extract training data from ChatGPT and other models. As far as other types of foundation models go, recent work by Gary Marcus and Reid Southern has shown that you can use Midjourney (text-to-image) to generate images such as these1:\n\n\n[Image from here]\n\nThis seems to be emerging as a feature, not a bug, and hopefully it’s obvious to you why they called their IEEE opinion piece Generative AI Has a Visual Plagiarism Problem. And the space is moving quickly: SORA, OpenAI’s text-to-video model, is yet to be released and has already taken the world by storm."
  },
  {
    "objectID": "posts/pierre-menard/index.html#compression-transformation-hallucination-and-generation",
    "href": "posts/pierre-menard/index.html#compression-transformation-hallucination-and-generation",
    "title": "ChatGPT, Author of The Quixote",
    "section": "Compression, Transformation, Hallucination, and Generation",
    "text": "Compression, Transformation, Hallucination, and Generation\nTraining data isn’t stored in the model per se but large chunks of it are reconstructable, given the correct key (“prompt”).\nThere are lots of conversations about whether or not LLMs (and machine learning, more generally) are forms of compression or not. In many ways, they are, but they also have generative capabilities that we don’t often associate with compression.\nTed Chiang wrote a thoughtful piece for the New Yorker called ChatGPT is a Blurry JPEG of the Web that opens with the analogy of a photocopier making a slight error due to the way it compresses the digital image. It’s an interesting piece that I commend to you but one that makes me uncomfortable. To me, the analogy breaks down before it begins: firstly, LLMs don’t merely blur, but perform highly non-linear transformations, which means you can’t just squint and get a sense of the original; secondly, for the photocopier, the error is a bug, whereas, for LLMs, all errors are features. Let me explain. Or, rather, let Andrej Karpathy explain:\n\nI always struggle a bit [when] I’m asked about the “hallucination problem” in LLMs. Because, in some sense, hallucination is all LLMs do. They are dream machines.\n\n\nWe direct their dreams with prompts. The prompts start the dream, and based on the LLM’s hazy recollection of its training documents, most of the time the result goes someplace useful.\n\n\nIt’s only when the dreams go into deemed factually incorrect territory that we label it a “hallucination”. It looks like a bug, but it’s just the LLM doing what it always does.\n\n\nAt the other end of the extreme consider a search engine. It takes the prompt and just returns one of the most similar “training documents” it has in its database, verbatim. You could say that this search engine has a “creativity problem” - it will never respond with something new. An LLM is 100% dreaming and has the hallucination problem. A search engine is 0% dreaming and has the creativity problem.\n\nAs a side note, building products that strike balances between Search and LLMs will be a highly productive area and companies such as Perplexity AI are also doing interesting work there.\nIt’s interesting to me that, while LLMs are constantly “hallucinating”2, they can also reproduce large chunks of training data, not just go “someplace useful”, as Karpathy put it (summarization, for example). So: is the training data “stored” in the model? Well, no, not quite. But also…. Yes?\nLet’s say I tear up a painting into a thousand pieces and put them back together in a mosaic: is the original painting stored in the mosaic? No, unless you know how to rearrange the pieces to get the original. You need a key. And, as it turns out, there happen to be certain prompts that act as keys that _unlock _training data (for insiders, you may recognize this as extraction attacks, a form of adversarial machine learning).\nThis also has implications for whether Generative AI can create anything particularly novel: I have high hopes that it can but I think that is still yet to be demonstrated. There are also significant and serious concerns about what happens when we continually train models on the outputs of other models."
  },
  {
    "objectID": "posts/pierre-menard/index.html#implications-for-copyright-and-legitimacy-big-tech-and-informed-consent",
    "href": "posts/pierre-menard/index.html#implications-for-copyright-and-legitimacy-big-tech-and-informed-consent",
    "title": "ChatGPT, Author of The Quixote",
    "section": "Implications for Copyright and Legitimacy, Big Tech and Informed Consent",
    "text": "Implications for Copyright and Legitimacy, Big Tech and Informed Consent\nCopyright isn’t the correct paradigm to be thinking about here; legal doesn’t mean legitimate; surveillance models trained on photos of your children.\nNow I don’t think this necessarily has implications for whether LLMs are infringing copyright and whether ChatGPT is infringing that of the NYTimes, Sarah Silverman, George RR Martin, or any of us whose writing has been scraped for training data. But I also don’t think copyright is necessarily the best paradigm for thinking through whether such training and deployment should be legal or not. Firstly, copyright was created in response to the affordances of mechanical reproduction and we now live in an age of digital reproduction, distribution, and generation. It’s also about what type of society we want to live in collectively: copyright itself was originally created to incentivize certain modes of cultural production.\nEarly predecessors of modern copyright law, such as the Statute of Anne (1710) in England, were created to incentivize writers to write and to incentivize more cultural production. Up until this point, the Crown had granted exclusive rights to print certain works to the Stationers’ Company, effectively creating a monopoly, and there weren’t financial incentives to write. So, even if OpenAI and their frenemies aren’t breaching copyright law, what type of cultural production are we and aren’t we incentivizing by not zooming out and looking at as many of the externalities here as possible?\nRemember the context. Actors and writers were recently striking while Netflix had an AI product manager job listing with a base salary ranging from $300K to $900K USD3. Also, note that we already live in a society where many creatives end up in advertising and marketing. These may be some of the first jobs on the chopping block due to ChatGPT and friends, particularly if macroeconomic pressure keeps leaning on us all. And that’s according to OpenAI!\n\nBack to copyright: I don’t know enough about copyright law but it seems to me as though LLMs are “transformative” enough to have a fair use defense in the US. Also, training models doesn’t seem to me to infringe copyright because it doesn’t yet produce output! But perhaps it should infringe something: even when the collection of data is legal (which statistically it won’t entirely be for any web-scale corpus), it doesn’t mean it’s legitimate, and it definitely doesn’t mean there was informed consent.\nTo see this, let’s consider another example, that of MegaFace. In How Photos of Your Kids Are Powering Surveillance Technology, the NYTimes reported that\n\nOne day in 2005, a mother in Evanston, Ill., joined Flickr. She uploaded some pictures of her children, Chloe and Jasper. Then she more or less forgot her account existed…\n\n\nYears later, their faces are in a database that’s used to test and train some of the most sophisticated [facial recognition] artificial intelligence systems in the world.\n\nWhat’s more,\n\nContaining the likenesses of nearly 700,000 individuals, it has been downloaded by dozens of companies to train a new generation of face-identification algorithms, used to track protesters, surveil terrorists, spot problem gamblers, and spy on the public at large.\n\nEven in the cases where this is legal (which seem to be the vast majority of cases), it’d be tough to make an argument that it’s legitimate and even tougher to claim that there was informed consent. I also presume most people would consider it ethically dubious. I raise this example for several reasons:\n\nJust because something is legal, doesn’t mean we want it to be going forward;\nThis is illustrative of an entirely new paradigm, enabled by technology, in which vast amounts of data can be collected, processed, and used to power algorithms, models, and products, the same paradigm under which GenAI models are operating;\nIt’s a paradigm that’s baked into how a lot of Big Tech operates and we seem to accept in many forms now: but if you’d built LLMs 10, let alone 20, years ago by scraping web-scale data, this would likely be a very different conversation.\n\nI should probably also define what I mean by “legitimate/illegitimate” or at least point to a definition. When the Dutch East India Company “purchased” Manhattan from the Lenape people, Peter Minuit, who orchestrated the “purchase”, supposedly paid $24 worth of trinkets. That wasn’t illegal. Was it legitimate? It depends on your POV: not from mine. The Lenape didn’t have a conception of land ownership, just as we don’t yet have a serious conception of data ownership. This supposed “purchase” of Manhattan has resonances with uninformed consent. It’s also relevant as Big Tech is known for its extractive and colonialist practices."
  },
  {
    "objectID": "posts/pierre-menard/index.html#this-isnt-about-copyright-the-nytimes-or-openai",
    "href": "posts/pierre-menard/index.html#this-isnt-about-copyright-the-nytimes-or-openai",
    "title": "ChatGPT, Author of The Quixote",
    "section": "This isn’t about copyright, the NYTimes, or OpenAI ",
    "text": "This isn’t about copyright, the NYTimes, or OpenAI \nIt’s about what type of society you want to live in\nI think it’s entirely possible that the NYTimes and OpenAI will settle out of court: OpenAI has strong incentives to do so and the Times likely also has short-term incentives to. However, the Times has also proven itself adept at playing the long game. Don’t fall into the trap of thinking this is merely about the specific case at hand. To zoom out again, we live in a society where mainstream journalism has been carved out and gutted by the Internet, Search, and Social Media. The NYTimes is one of the last serious publications standing and they’ve worked incredibly hard and cleverly in their “digital transformation” since the advent of the internet.4\nPlatforms such as Google have inserted themselves as middlemen between producers and consumers in a manner that has killed the business models of many of the content producers. They’re also disingenuous about what they’re doing: when the Australian Government was thinking of making Google pay news outlets that it linked to in Search, Google’s response was:\n\nNow remember, we don’t show full news articles, we just show you where you can go and help you to get there. Paying for links breaks the way search engines work, and it undermines how the web works, too. Let me try and say it another way. Imagine your friend asks for a coffee shop recommendation. So you tell them about a few nearby so they can choose one and go get a coffee. But then you get a bill to pay all the coffee shops, simply because you mentioned a few. When you put a price on linking to certain information, you break the way search engines work, and you no longer have a free and open web. We’re not against a new law, but we need it to be a fair one. Google has an alternative solution that supports journalism. It’s called Google News Showcase.\n\nLet me be clear: Google has done incredible work in “organizing the world’s information” but here they’re disingenuous in comparing themselves to a friend offering advice on coffee shops: friends don’t tend to have global data, AI, and infrastructural pipelines, nor are they business predicated on surveillance capitalism.\nCopyright aside, the ability of Generative AI to displace creatives is a real threat and I’m asking a real question: do we want to live in a society where there aren’t many incentives for humans to write, paint, and make music? Borges may not write today, given current incentives. If you don’t particularly care about Borges, perhaps you care about Philip K. Dick, Christopher Nolan, Salman Rushdie, or the Magic Realists, who were all influenced by his work.\nBeyond all the human aspects of cultural production, don’t we also still want to dream? Or do we also want to outsource that and have LLMs do all the dreaming for us?"
  },
  {
    "objectID": "posts/pierre-menard/index.html#notes",
    "href": "posts/pierre-menard/index.html#notes",
    "title": "ChatGPT, Author of The Quixote",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "posts/pierre-menard/index.html#footnotes",
    "href": "posts/pierre-menard/index.html#footnotes",
    "title": "ChatGPT, Author of The Quixote",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s ironic that, when syndicating this essay on O’Reilly Radar, we didn’t reproduce the images from Marcus’ article because we didn’t want to risk violating copyright–a risk that Midjourney apparently ignores and perhaps a risk that even IEEE and the authors took on!↩︎\nI’m putting this in quotation marks as I’m still not entirely comfortable with the implications of antropomorphizing LLMs in this manner.↩︎\nMy intention isn’t to suggest that Netflix is all bad. Far from it, in fact – Netflix has also been hugely powerful in providing a massive distribution channel to creatives across the globe. It’s complicated.↩︎\nAlso note that the outcome of this case could have significant impact for the future of OSS and open weight foundation models, something I hope to write about in future.↩︎"
  },
  {
    "objectID": "posts/building-llms/index.html",
    "href": "posts/building-llms/index.html",
    "title": "Developing and Training LLMs From Scratch",
    "section": "",
    "text": "In a recent episode of the Vanishing Gradients podcast, host Hugo Bowne-Anderson spoke with Sebastian Raschka, an AI researcher and educator, about the full lifecycle of large language models (LLMs). The conversation covers a wide range of topics related to building, training, fine-tuning, and deploying LLMs. Read this post to check out what they covered and you can watch the full episode below. We’ve also embedded short clips in the relevant sections of this post.\nTLDR:\nNote: we used Claude Opus to help write this post, based on the podcast transcript.\nIf you’re interested in getting hands-on, you can find a reproducible run-down of Sebastian live coding to fine-tune GPT-2 here."
  },
  {
    "objectID": "posts/building-llms/index.html#llm-lifecycle",
    "href": "posts/building-llms/index.html#llm-lifecycle",
    "title": "Developing and Training LLMs From Scratch",
    "section": "LLM Lifecycle",
    "text": "LLM Lifecycle\n\n\n\n\nThe LLM lifecycle may seem like a big, intimidating term, but it can be broken down into several key steps:\n\nCoding the model architecture: Choose a base architecture (e.g., GPT, Llama, Phi) and define unique properties like:\n\nVocabulary size\nEmbedding size\nNumber of attention heads and transformer blocks\nActivation functions\nThese choices determine the size of the LLM, the data it needs, and the compute requirements for training and deployment.\n\nPre-training: Train the model on a large corpus of text data, which can be either:\n\nGeneral (e.g., for models like Llama, Gemma, and Phi)\nDomain-specific (e.g., finance-focused data for BloombergGPT)\nThe tradeoff is whether the expense of custom pre-training, which can cost hundreds of thousands to millions of dollars, is worth it for the specific use case versus fine-tuning one of the general models.\n\nFine-tuning: Adapt the pre-trained model for a specific task or domain. Options include:\n\nInstruction fine-tuning: Make the model better at following instructions without changing its underlying knowledge\nTask-specific fine-tuning: Train the model for a downstream task like spam classification\nFine-tuning is usually not as effective as pre-training for instilling entirely new knowledge into the model. For example, fine-tuning an English LLM on Spanish data may improve its Spanish performance if there was already some Spanish data during pre-training, but it likely won’t be as good as pre-training from scratch on Spanish.\n\nRetrieval Augmented Generation (RAG): If the goal is to have the model retrieve information from documents rather than generating the full response from scratch, RAG can be used on top of a pre-trained LLM. This is useful when you want the model to draw on external knowledge to answer questions or summarize information.\nDeployment: Deploy the model to production and monitor its performance, making updates as needed. This may involve serving the model through an API, integrating it into a user-facing application, and setting up monitoring and logging to track its behavior over time.\n\nDepending on the use case, not all of these steps may be necessary. For example, a simple personal assistant could potentially just use an off-the-shelf model like Llama 3 without any custom fine-tuning, running locally or deployed with a basic UI."
  },
  {
    "objectID": "posts/building-llms/index.html#rlhf-and-other-add-ons",
    "href": "posts/building-llms/index.html#rlhf-and-other-add-ons",
    "title": "Developing and Training LLMs From Scratch",
    "section": "RLHF and Other Add-Ons",
    "text": "RLHF and Other Add-Ons\n\n\n\n\nBeyond the core LLM lifecycle, there are some interesting extensions and add-ons to consider. One hot area of research is reinforcement learning from human feedback (RLHF), which aims to align LLMs more closely with human preferences.\nThe RLHF process typically involves two main stages:\n\nSupervised Instruction Fine-Tuning:\n\nInstruction fine-tune the pre-trained model (using next-token prediction on instruction data) on a large dataset (e.g., 50K examples)\nGoal is to make the model better at following instructions and generating relevant outputs\n\nReward Modeling and Policy Optimization:\n\nCollect a smaller dataset of human preferences, where each example includes:\n\nTwo possible model outputs for the same prompt\nA human annotation indicating which output is preferred\n\nTrain a reward model to predict the human-preferred output based on this dataset\nOptimize the main LLM (the “policy”) to maximize the reward predicted by the reward model\n\n\nSebastian illustrates the impact of RLHF with an example: if an instruction-tuned model frequently uses the word “delve” compared to its non-instruction-tuned counterpart, it suggests that the human raters preferred outputs containing “delve” during the RLHF process.\nAnother interesting direction mentioned in the conversation is Continued Pre-training, advocated by Jeremy Howard and others. The idea is to continue pre-training the model on a smaller, task-specific dataset, rather than switching to supervised fine-tuning. This may help the model acquire new knowledge more effectively compared to standard fine-tuning."
  },
  {
    "objectID": "posts/building-llms/index.html#skills-you-need-to-work-with-llms",
    "href": "posts/building-llms/index.html#skills-you-need-to-work-with-llms",
    "title": "Developing and Training LLMs From Scratch",
    "section": "Skills You Need to Work with LLMs",
    "text": "Skills You Need to Work with LLMs\n\n\n\n\nWorking with LLMs requires a foundation in deep learning, but a lot of the core concepts carry over:\n\nFamiliarity with PyTorch (or another deep learning framework)\nUnderstanding of loss functions, optimizers, and evaluation metrics\nComfort with training loops, including concepts like cross-entropy loss, learning rate schedules, gradient descent, and backpropagation\n\nSebastian notes that you can think of an LLM as a big PyTorch model – if you’re comfortable with the core deep learning concepts, you’re already well on your way to working with LLMs.\nHowever, the scale of LLMs introduces some additional challenges:\n\nMulti-GPU and multi-node training: To train LLMs efficiently, you’ll need to use techniques like model parallelism, pipeline parallelism, and sharded data parallelism across multiple GPUs and multiple nodes (depending on the model size).\nLow-precision and mixed-precision training: Using lower-precision formats like bfloat16 can help reduce memory requirements and speed up training.\nCheckpointing: Saving model checkpoints periodically is crucial to avoid losing progress if a long-running training job fails.\n\nEngineering best practices become even more important when working with LLMs. Tools like PyTorch Lightning and Fabric can help abstract away some of the complexity of distributed training, but it’s still valuable to understand what’s going on under the hood."
  },
  {
    "objectID": "posts/building-llms/index.html#hardware-resources-to-work-with-llms",
    "href": "posts/building-llms/index.html#hardware-resources-to-work-with-llms",
    "title": "Developing and Training LLMs From Scratch",
    "section": "Hardware / Resources to Work with LLMs",
    "text": "Hardware / Resources to Work with LLMs\n\n\n\n\nTraining LLMs requires significant compute resources beyond a typical laptop or desktop. Sebastian outlines a few common options:\n\nGoogle Colab: Offers free GPU access for prototyping small models, but with strict runtime limits. Not suitable for larger-scale training.\nCloud platforms (AWS, GCP, Azure): Allow you to rent powerful GPU instances on-demand, but require careful environment setup and cost management. You’ll need to set up your environment from scratch each time, which can be time-consuming.\nManaged platforms (e.g., Lightning AI): Provide streamlined development environments with easy access to pre-configured compute resources. The price is similar to AWS, but it can save a lot of engineering time. Lightning AI also offers persistent storage, so you don’t need to constantly re-install your dependencies.\n\nWhen choosing a hardware setup, key considerations include the amount of GPU RAM available (often the main bottleneck for LLMs), support for multi-GPU parallelism techniques like tensor parallelism and sharded data parallelism, and fast interconnects between devices.\nSebastian emphasizes the importance of starting small and debugging your code on a local machine before scaling up. He likes to prototype using compact models like GPT-2 or smaller versions of Pythia, which can run on a single CPU or GPU and shares many architectural similarities with larger LLMs. In fact, he was able to train a spam classifier using GPT-2 on his MacBook Air in just 6 minutes (or 30 seconds on a GPU)!\nInterestingly, many of the most popular LLM architectures (Llama, Mistral, Phi, etc.) are derived from the same basic building blocks as GPT. So getting hands-on experience with GPT-2, even on a small scale, can teach you a lot about how LLMs work under the hood. Playing with pre-trained weights is also a great way to validate your code, since the model will only generate sensible outputs if you’ve implemented everything correctly."
  },
  {
    "objectID": "posts/building-llms/index.html#prompt-engineering-rag-or-fine-tuning",
    "href": "posts/building-llms/index.html#prompt-engineering-rag-or-fine-tuning",
    "title": "Developing and Training LLMs From Scratch",
    "section": "Prompt Engineering, RAG, or Fine-Tuning?",
    "text": "Prompt Engineering, RAG, or Fine-Tuning?\n\n\n\n\nWhen working with LLMs, there are several approaches to getting the desired outputs for your specific use case. Sebastian breaks down the key considerations for choosing between prompt engineering, retrieval augmented generation (RAG), and fine-tuning.\n\nPrompt engineering: The simplest approach, where you craft prompts to elicit the desired outputs from the model.\n\nWorks well when the model has the necessary knowledge and just needs guidance.\nLimited by the model’s pre-existing knowledge and ability to follow instructions.\n\nRetrieval Augmented Generation (RAG): Enhances the model’s knowledge by retrieving relevant documents from a corpus based on the user’s prompt.\n\nUseful when you have a large corpus of relevant data that wasn’t included in the model’s original training.\nRequires a well-organized corpus and infrastructure for efficient retrieval.\n\nFine-tuning: Trains the model on a specific dataset to deeply internalize new knowledge or adapt to a specific task.\n\nMost powerful approach, but also the most computationally intensive and time-consuming.\nRequires significant compute resources and high-quality data.\n\n\nSo how do you choose the right approach for your use case? Sebastian recommends the following:\n\nStart with prompt engineering and gradually work your way up to more advanced techniques as needed.\nIf prompt engineering isn’t sufficient, try incorporating RAG to expand the model’s knowledge base.\nIf you find yourself using RAG frequently for the same task, or if the model is still struggling, consider investing in fine-tuning.\n\nOf course, the feasibility of each approach also depends on your resources and constraints. Prompt engineering is cheap and easy, RAG requires a well-organized corpus, and fine-tuning is the most resource-intensive.\nThe key is to experiment and iterate:\n\nStart with the simplest approach that might work, and gradually add complexity as needed.\nDon’t be afraid to combine techniques (e.g., using prompt engineering to improve RAG queries or fine-tuning a model and then using prompt engineering to guide its outputs).\n\nAs you gain more experience working with LLMs, you’ll develop a better intuition for which approaches are likely to work best in different scenarios. But as a general rule, Sebastian recommends starting simple and only moving to more advanced techniques when the benefits clearly outweigh the costs."
  },
  {
    "objectID": "posts/building-llms/index.html#llm-research-techniques-lora-dpo-and-more",
    "href": "posts/building-llms/index.html#llm-research-techniques-lora-dpo-and-more",
    "title": "Developing and Training LLMs From Scratch",
    "section": "LLM Research Techniques: LoRA, DPO, and more",
    "text": "LLM Research Techniques: LoRA, DPO, and more\n\n\n\n\nAmong the many exciting research ideas in the LLM space, Sebastian highlights a few of his favorites:\n\nLoRA (Low-Rank Adaptation): An efficient fine-tuning technique that approximates full parameter updates with smaller updates to a low-rank decomposition of the weights. LoRA has been around for a couple years but remains Sebastian’s go-to for fine-tuning due to its strong performance and efficiency. He’s also excited about recent variants like QLoRA and DoRA.\nMixture of Experts (MoE): An approach to conditional computation where different subsets of the model parameters are activated depending on the input. This can make the model more efficient by avoiding unnecessary computation. Sebastian notes that MoE is used in some recent state-of-the-art models like Mixtral.\nMulti-token prediction: A technique for speeding up LLM inference by predicting multiple tokens at once using separate output heads. Sebastian describes this as a clever “hack” that achieves a 4x inference speedup without any fundamentally new math, showing the power of creatively combining existing building blocks.\nDirect Preference Optimization (DPO): An alternative to RLHF that removes the need to train a separate reward model, instead directly optimizing the policy (i.e., the LLM) directly to make its outputs preferred by humans. DPO is simpler than RLHF, which may explain why many top-performing models on leaderboards use it. However, a recent paper suggests that RLHF with PPO still outperforms DPO, albeit with more complexity.\n\nLooking ahead, Sebastian is excited to see more research combining multiple techniques, like the forthcoming Llama 3 model which uses both RLHF and DPO. He also mentions DoRA (an extension of LoRA), the Kahneman Tversky Optimization (a preference-free form of RLHF), and other promising ideas. While the field is moving quickly, some of these core techniques seem to be standing the test of time so far."
  },
  {
    "objectID": "posts/building-llms/index.html#stack-overflow-and-openai",
    "href": "posts/building-llms/index.html#stack-overflow-and-openai",
    "title": "Developing and Training LLMs From Scratch",
    "section": "Stack Overflow and OpenAI",
    "text": "Stack Overflow and OpenAI\nThe recent collaboration between Stack Overflow and OpenAI raises interesting questions about the future of community-driven knowledge sharing in the age of LLMs.\nOn one hand, there’s a risk that if everyone starts relying on LLMs like ChatGPT to answer their coding questions, they’ll stop contributing to resources like Stack Overflow. This could lead to a negative feedback loop where the lack of new content degrades the quality of the LLMs themselves, since they rely heavily on sites like Stack Overflow for training data.\nAt the same time, LLMs are particularly well-suited for answering common, repeated questions that make up a significant fraction of Stack Overflow traffic. If these questions can be reliably answered by an LLM, it may free up human experts to focus on more novel, challenging problems. In this future, Stack Overflow could become a site focused on cutting-edge content and high-quality discussions, while LLMs handle the long tail of simpler queries.\nSebastian emphasizes that the success of Stack Overflow and other community-driven resources in the age of LLMs will depend on their ability to adapt and provide unique value that can’t be easily replicated by models. This could mean doubling down on moderation and quality control, investing in new content formats and interaction models, or exploring hybrid human-AI collaboration tools.\nIn the end, LLMs are only as good as the data they’re trained on – which comes from the collective knowledge contributions of humans. Finding ways to sustain and encourage those contributions, even as LLMs become more prevalent, will be a key challenge and opportunity going forward.\nBe sure to check out Sebastian’s book “Build a Large Language Model (From Scratch)” and his other tutorials to dive deeper into the technical details of the LLM lifecycle!\nIf you enjoyed this, you can follow Sebastian on twitter here and Hugo here. You can also subscribe to Hugo’s fortnightly Vanishing Gradients Newsletter here."
  },
  {
    "objectID": "posts/gen-ai-atomic-units/index.html",
    "href": "posts/gen-ai-atomic-units/index.html",
    "title": "Getting Started with Generative AI for Everyone",
    "section": "",
    "text": "Tl;dr\nWith the advent of Generative AI, more people than ever (technical and non-technical) can build interesting, fun, and productivity-increasing AI apps and workflows. There are several challenges:\nIn this blog post, we lay out many of the current capabilities of genAI tools, suggest ways to combine them, and spell out the genAI mindset of combining atomic units. In addition, we propose a simple protocol for building such apps and workflows:\nWhether it’s YouTube video summarization, generating images with captions, building workflow automation tools, generating email templates, or writing short stories accompanied by images, we hope that people can use this protocol to build apps and workflows that help them at work and in their daily lives.\nThis post is intended for those familiar with the basic capabilities of LLMs such as ChatGPT and Claude. For those who want to find out more about how such LLMs can be used to help with the following, check this post out:\nIt was great to co-write this post with Johno Whitaker!"
  },
  {
    "objectID": "posts/gen-ai-atomic-units/index.html#the-genai-mindset-combining-atomic-units",
    "href": "posts/gen-ai-atomic-units/index.html#the-genai-mindset-combining-atomic-units",
    "title": "Getting Started with Generative AI for Everyone",
    "section": "The GenAI mindset: combining atomic units",
    "text": "The GenAI mindset: combining atomic units\nWe recently released an episode of Vanishing Gradients about accessibility in generative AI. Both Johno and Hugo are particularly excited about how all the new tooling allows non-technical people to use machine learning and AI models through the ability to interact with them using natural language.\nThere are also new elements of how we even think about working with such tools. Consider the different mindsets of how we build machine learning, deep learning, and AI models:\n\nML mindset: how can I generate useful features for prediction?\nDL mindset: how can I express what I want in terms of some loss function to optimize?\nGenAI mindset: how can I build applications using atomic generative AI units?\n\nFor an example of the latter, consider a task Hugo does a lot of in his day job: summarizing YouTube videos (see here and here, for examples). To do this, you can break the task down into atomic units:\n\nSpeech-to-text: to get a transcript of the video;\nText-to-text: to summarize the transcript.\n\nNow anybody with differing levels of technical and hacker prowess can do this:\n\nIf you’re into writing Python code, accessing cloud compute, and playing around with foundation models, you could use OpenAI’s OSS speech-to-text Whisper model and then any number of OSS LLMs for summarization, e.g. Llama2 or Mixtral;\nIf you don’t want to write any code, you could use products such as Otter.ai or Descript to get the transcription and then use Claude or ChatGPT for the summarization;\nIf you’re lazy like Hugo, you may have discovered that some legend has built a Chrome plugin called Glasp that essentially does this for you: Glasp performs the speech-to-text and pipes the result into Claude or ChatGPT, which then performs the text-to-text summarization."
  },
  {
    "objectID": "posts/gen-ai-atomic-units/index.html#whats-currently-possible-with-genai",
    "href": "posts/gen-ai-atomic-units/index.html#whats-currently-possible-with-genai",
    "title": "Getting Started with Generative AI for Everyone",
    "section": "What’s currently possible with GenAI",
    "text": "What’s currently possible with GenAI\nWe have a public awareness issue: many people don’t even know what’s currently possible and what the atomic units are! For example, you can speak to chatGPT and have it create images, which combines speech-to-text and text-to-image. So what type of models are there?\n\nText-to-text, such as ChatGPT, Claude, and so many open-source options;\nText-to-speech and speech-to-text, such as Whisper, Otter.ai, and Descript;\nText-to-image, such as Stable Diffusion, Midjourney, Pika Labs, Runway ml, and DALL·E 3;\nImage-to-image, such as Runway ml;\nText-to-video, such as Stable Video Diffusion, Runway ml, and Sora;\nImage-to-video, such as Stable Video Diffusion and Runway ml;\nText-to-music, such as Suno AI.\n\nHugo generated this list using ChatGPT and you can see the full conversation for other ideas here. It’s worth mentioning that the examples above sun the gamut from free to paid to open source, so feel free to jump in and play around with all types of models. Also note that some of these types of models are fairly obvious and single-use (text-to-speech or image) while others are a lot more configurable (e.g., LLMs can be used for semi-arbitrary text transformations, such as summarization, conversation, and more)."
  },
  {
    "objectID": "posts/gen-ai-atomic-units/index.html#an-example-of-combining-atomic-ai-units",
    "href": "posts/gen-ai-atomic-units/index.html#an-example-of-combining-atomic-ai-units",
    "title": "Getting Started with Generative AI for Everyone",
    "section": "An example of combining atomic AI units",
    "text": "An example of combining atomic AI units\nIn their conversation, Johno gives a nice example: he wants to programmatically pull an image from the internet, then use a vision and text model to generate a silly quote to add to it:\n\n\n\n\n\nGet image: He first asked ChatGPT how he could use Python to pull a random high-quality photo from Unsplash – ChatGPT gave him some code and instructions, which worked;\nGet quote: He used OpenAI’s docs and quickstart examples to get an image-to-text model to generate a quote;\nCombine quote and image: He asked ChatGPT to give him Python code to do this and it delivered!\n\nNote that Steps 1 and 3 don’t actually require any AI models but Johno cleverly used ChatGPT to give him the code for these steps.\nSo why are we telling you all of this? Well, we want to encourage everyone to experiment with all of these burgeoning tools, both for productivity and for pleasure!"
  },
  {
    "objectID": "posts/gen-ai-atomic-units/index.html#how-to-get-started-building-your-own-genai-apps-and-workflows",
    "href": "posts/gen-ai-atomic-units/index.html#how-to-get-started-building-your-own-genai-apps-and-workflows",
    "title": "Getting Started with Generative AI for Everyone",
    "section": "How to get started building your own GenAI apps and workflows",
    "text": "How to get started building your own GenAI apps and workflows\nThe protocol we suggest is as follows:\n\nIdentify a basic MVP of what you want to build (e.g. YouTube video summarization, generating images with captions, building workflow automation tools, generating email templates, or writing short stories accompanied by images);\nBreak it down into atomic units;\nDecide on a tool for each atomic unit;\nStitch them together!\n\nWe also strongly encourage you to speak with an AI assistant such as ChatGPT or Claude about any of these steps also, even merely to ideate about Step 1!\nWe now suggest a few ways to get started with this process, for those who don’t code, for those who, and for those who want to chat with an AI to get some inspiration :)\nNote: Although currently we are stitching together a variety of tools and models, we are currently seeing the rise of more and more multimodal tools, which are systems or platforms that can process and integrate multiple types of data or input modalities, such as text, images, audio, and video. It’s eminently possible and perhaps likely that we’ll be using such tools for many steps of this process in the future (as opposed to stitching many tools together), but the mindset will remain the same.\n\nExplore what’s possible without code on Replicate\nGo to Replicate’s explore page and, well, explore! Hugo is just looking now and it’s all so much fun:\n\nYou have a sticker maker, high-quality image generator, a way to play with Google’s Gemma model, a Mixtral assistant, a fast Whisper model, and an image-to-image model which, once you click through, shows you how to turn this image\n\nInto this!\n\n\n\nExplore what’s possible with code\nDo the same but explore HuggingFace models and HuggingFace spaces. Something you can do here is look at the code for existing spaces for inspiration - in many cases you can copy-paste (or git clone the whole space) to get a working starting point. Many of them also have Gradio demos you can play around with immediately!\n\n\nChat with ChatGPT or Claude about what’s possible\nSeriously. You saw above how Johno chatted with ChatGPT to achieve a basic task. Have a chat with an LLM about how to do things you’d like to. If you can’t think of anything, ask it to help you think of something! You can even ask it to ask you about your interests and to help you think about what you could build.\nHugo actually just played around with ChatGPT to do something along these lines and ended up writing a story and generating an image to accompany it with ChatGPT. You can check out the conversation here, if you’d like, and the resulting notebook here :)\nThere are a few things to note about the conversation Hugo had with ChatGPT here:\n\nHugo made some slight prompting errors by not being specific enough such as writing “yes lets go through it step by step please” instead of “let’s start with step 1 now”;\nSeveral models ChatGPT suggested didn’t end up working! Hugo had to probe ChatGPT in order to get the correct code: this is common! Just like conversations with people, it’s not uncommon to have to ask several times (to have a conversation) to get the information you’re really looking for;\nHugo likes writing code in Jupyter notebooks so suggested to ChatGPT that they use notebooks and Python code: feel free to suggest what makes you most comfortable (but also try to push yourself!).\n\nTo the final point, a large part of the podcast conversation is about how people who don’t write code can leverage GenAI models these days. Having said that, we encourage people who don’t code to learn a little, if the feel like it, and LLMs make it easier than ever to do so.\nJensen Huang, CEO of Nvidia, “argues that we should stop saying kids should learn to code. He argues the rise of AI means we can replace programming languages with human language prompts thus enabling everyone to be a programmer.”\n\n\n\nJensen Huang, CEO of Nvidia, argues that we should stop saying kids should learn to code. He argues the rise of AI means we can replace programming languages with human language prompts thus enabling everyone to be a programmer. AI will kill coding.pic.twitter.com/SxK9twhEby\n\n— Dare Obasanjo🐀 (@Carnage4Life) February 24, 2024\n\n\n\nWe don’t necessarily agree with this characterization. Not everybody needs to be a software engineer but learning a bit of coding pays serious dividends and likely will in the future, even just to stitch atomic GenAI units together.\n\n\nTell us what you did\nWe’d be excited to hear from you about what you were able to build or even just play around with! Feel free to ping Hugo on Twitter @hugobowne to let him know :)"
  },
  {
    "objectID": "posts/llm-productivity/index.html",
    "href": "posts/llm-productivity/index.html",
    "title": "Boost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started",
    "section": "",
    "text": "Tl;dr\nLarge Language Models (LLMs), such as ChatGPT and Claude, are known for their text generation and conversational abilities. But they are also good at other tasks that can help time-starved non-technical working professionals save time, such as text summarization and analysis.\nHave a 60-page PDF you don’t have time to read and want summarized? A meeting you need action items from? Or an online video you need transcribed and summarized? Would you like them to then generate a PDF, text file, or spreadsheet of the results?\nChatGPT will do these tasks for you. It can also do market research, competitive analyses, email drafting, ad campaign ideation, and much more. This blog post will take you through many such examples, including relevant conversations with ChatGPT: the goal is to give you the tools to get LLMs to do what they’re good at, freeing you up to do what you’re good at.\nNote: I used GPT-4 in ChatGPT for much of this post. If I were to rewrite it (which I might), I’d use Claude Opus, as I’ve got better results with it recently. And yes, I’m subscribed to far too many LLMs and, yes, it feels like being subcribed to too many streaming services right now! Also, I usually write for ML & AI developers but I realised a lot of people don’t know how LLMs like Claude and ChatGPT can help them with basic knowledge work so that was the rationale for this post."
  },
  {
    "objectID": "posts/llm-productivity/index.html#things-chatgpt-can-do-that-perhaps-you-dont-know-about",
    "href": "posts/llm-productivity/index.html#things-chatgpt-can-do-that-perhaps-you-dont-know-about",
    "title": "Boost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started",
    "section": "Things ChatGPT can do that perhaps you don’t know about",
    "text": "Things ChatGPT can do that perhaps you don’t know about\nLLMs offer lots of potential “productivity hacks” for working professionals:\n\nMeeting Summarization and Action Item Generation: After recording a meeting or providing notes, generative AI can summarize the key discussions, decisions, and action items.\nCompetitive Analysis and Market Research: By feeding generative AI tools with queries and data sources, they can conduct preliminary market research and competitive analysis, providing summaries of market trends, competitor strategies, and potential opportunities.\nAutomated Email Drafting and Management: Generative AI can help draft emails based on brief inputs, saving time on composing responses or outreach messages. For instance, by summarizing the key points you wish to communicate, the AI can generate a well-crafted email.\nProject Proposal and Report Generation: Generative AI can help draft project proposals and reports by inputting data points and objectives, significantly reducing the time required for these tasks. This includes generating structured documents with executive summaries, analyses, and conclusions based on the provided data, which can be further customized as needed.\nContent Creation and Marketing Material Development: Generative AI can assist in creating high-quality content, such as blog posts, social media content, and marketing materials, with minimal input. By providing a brief outline or key points, the AI can generate drafts that can be fine-tuned, enabling professionals to maintain an active online presence without dedicating extensive time to content creation.\n\nLet’s now look at summarization, move on to analysis, and then combine these with the better-known generative capabilities of LLMs."
  },
  {
    "objectID": "posts/llm-productivity/index.html#summarization",
    "href": "posts/llm-productivity/index.html#summarization",
    "title": "Boost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started",
    "section": "Summarization",
    "text": "Summarization\nLLMs will summarize lots of things you may not have the time to read, such as PDFs, blog posts, websites, and YouTube videos.\n\nSummarizing documents\nIf you want to summarize a document you don’t have time to read, you can upload it to ChatGPT. Here’s an example. Have a look and notice that you can get the summary in any number of forms. For example, I asked it for\n\nA basic summary\nAn executive summary appropriate for business leaders\nAn ELI5 (explain it like I’m 5) summary\n\n\n\n\nSummarizing blog posts and websites\nWe can also summarize websites such as Andrei Karpathy’s seminal Software 2.0 blog post. Note that I had to nudge ChatGPT to do this, however! Don’t be afraid to do this: it’s a conversation.\n\nWe can also take it a step further and get ChatGPT to recommend popular essays by someone we’re interested in and then summarize some of the most popular ones.\n\nIn this example, I also got ChatGPT to generate a PDF of its summaries for me, in case I wanted to read later or share.\n\n\nSummarizing meetings, YouTube videos, and so on\nYou can also summarize anything else that you have in text. One of the big time savers for many is meeting summarization and action item generation. To do this in ChatGPT, you need to get the meeting audio/video into text form:\n\nif you record meetings using Zoom, you can “enable transcription” in Zoom;\nIf you share videos with colleagues using Slack, it will generate a transcript for you;\nYou can upload audio and/or video to services such as Descript and Otter.ai to generate transcripts.\n\nYou can then upload your transcript to Claude or ChatGPT to generate a summary and action items. You can also ask your LLM specific questions to drill down into next steps for yourself, for example!\nA couple of notes:\n\nDepending on the length of your transcript, you may need to divide it into chunks: you can even ask ChatGPT how many words it will generally accept and even negotiate with it ;)\nTranscription services, such as Otter.ai mentioned above, already offer AI-generated summaries – this will become the norm! Currently, the most useful options I’ve found for customizability involve uploading transcripts into ChatGPT but I’d be somewhat surprised if this remained the case for too long.\n\nSo how about summarizing online videos?\n\nFirst, I need to get the transcript into ChatGPT. I suppose I could rip the video and use Descript to generate the transcript. Well, I actually used to do this.\nThen I discovered a Chrome extension called Glasp that transcribes YT videos directly into instances of Claude or ChatGPT for you: go get it!"
  },
  {
    "objectID": "posts/llm-productivity/index.html#analysis-of-texts",
    "href": "posts/llm-productivity/index.html#analysis-of-texts",
    "title": "Boost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started",
    "section": "Analysis of texts",
    "text": "Analysis of texts\nNext up, I have found LLMs useful for the analysis of texts. For example, I work in early-stage startup land in an almost absurdly busy space. So I’ve found using LLMs for market research and competitive analysis. I will give a toy example instead of examples from my day job."
  },
  {
    "objectID": "posts/llm-productivity/index.html#market-research-and-competitive-analysis",
    "href": "posts/llm-productivity/index.html#market-research-and-competitive-analysis",
    "title": "Boost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started",
    "section": "Market Research and Competitive Analysis",
    "text": "Market Research and Competitive Analysis\nLet’s say I wanted to start a business that provides an AI tutor for every math(s) student in the world. I told ChatGPT this and asked for help with a competitive analysis and market research.\nChatGPT came back immediately with many ideas for both, such as\n\nNotice it provided an analysis method and also suggested some competitors for us to consider. I decided to first embark on a direct competitor analysis and directed ChatGPT to perform such an analysis and provide the results to me in a table:\n\nIf you’re technically minded, also note that ChatGPT gives me the Python code that it used to generate this table, if I’d like it!\n\nI also thought it would be useful to have the analysis as a document I could share with colleagues so I asked ChatGPT to generate a spreadsheet for me, not being quite sure if it could and/or would: and it generated a .csv file! So I downloaded the .csv and put it in a Google sheet that you can find here.\nDo check out the relevant chat I had for more details, including some more market research, in which ChatGPT helps me with a back-of-the-envelope calculation to estimate the TAM (Total Addressable Market):"
  },
  {
    "objectID": "posts/llm-productivity/index.html#generation",
    "href": "posts/llm-productivity/index.html#generation",
    "title": "Boost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started",
    "section": "Generation",
    "text": "Generation\nText generation is one of the better-known capabilities of LLMs:\n\nEmail generation: give Claude or ChatGPT bullet points, and they can generate emails for you – play around with getting them to draft emails in different tones (e.g. to a manager, make it more formal, or informal)\nProject proposals and report generation: similarly, you can get LLMs to generate proposals and reports, based on talking points or even a conversation with the LLM – if you have a specific structure for the report, such as 3 sections X, Y, and Z, with maximum 500 words each, tell it and the LLM will generally make it happen!\nContent Creation and Marketing Material Development: particularly at the ideation stage, LLMs can be used for generating copy and images for all types of content creation.\n\nI encourage you to play around with the ones you find most useful!\n\nGenerating an ad campaign\nAs a fun and somewhat dystopic illustrative example, I asked ChatGPT to create a “marketing campaign for a product that inserts ads in people’s dreams.” it came up with the following:\n\nI also asked it to generate some potential marketing images for me:"
  },
  {
    "objectID": "posts/llm-productivity/index.html#what-to-do-next",
    "href": "posts/llm-productivity/index.html#what-to-do-next",
    "title": "Boost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started",
    "section": "What to do next",
    "text": "What to do next\nThe next steps are simple: think about what types of tasks that LLMs could make easier for you. Start by thinking along the lines of text summary, analysis, and generation. Then have conversations with ChatGPT and/or Claude to see what’s possible!\nSeveral notes to keep in mind:\n\nMy prediction is that the types of affordances discussed here will be increasingly embedded in products: e.g. you won’t need to go directly to ChatGPT but you’ll be able to do such tasks directly in a Microsoft Office product;\nYou may have noticed that ChatGPT required coaxing at some points in our conversations (at one point, it even said “I can’t do this”, I replied “Yes, it can” and it did!) – so persevere with it – it’s surprising what’s possible!\nIt is commonly accepted that ChatGPT got lazier for some time (some speculated that this happened last December and it may have been intentional as it’s when the internet itself gets lazier, with the holiday season – Sam Altman even came out in February and said ChatGPT should be much “less lazy now”; to this point, it’s important to note that ChatGPT has not been specifically designed for most of the tasks I’ve discussed in this post, but there are so many things that are possible;\nThis space is moving rapidly: I doubt we’ll be playing around with ChatGPT for a long time to achieve such tasks as discussed above; very many products will be released to solve many of these problems:\n\nwe have seen that Otter.ai summarizes transcripts;\nCopy.ai is doing interesting things for content creation and go-to-market strategies;\nMicrosoft Copilot is already incorporating a lot of the capabilities we’ve covered here but don’t be fooled by thinking there’s one obvious winner – this will likely end up more like the streaming wars;\n\nBe careful with your data! I wouldn’t currently upload any sensitive material to ChatGPT, for example: we really don’t know what OpenAI does with it; Copy.ai, for example, claims to use only LLMs that have zero-retention data policies; this space is going to be wild!\n\nSo the takeaway is simple: go play around and be more productive! Also, if you’re interested in learning about more capabilities of generative AI, check out my next blog post with Johno Whitaker on building GenAI apps that include a lot more than LLMs, such as those involving video, audio, and images.\n\nTell me what you did\nI’d be excited to hear from you about what you were able to build or even just play around with! Feel free to ping Hme on Twitter @hugobowne to let me know :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hugo's blog",
    "section": "",
    "text": "42 Lessons from a Year of Building with AI Systems\n\n\n\n\n\n\nGenAI\n\n\nLLMs\n\n\n\nFindings from a three-hour livestream and two podcast episodes about lessons from building real-world applications on top of LLMs.\n\n\n\n\n\nJul 1, 2024\n\n\nHugo Bowne-Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tuning GPT-2 for Spam Classification: A Live Coding Session with Sebastian Raschka\n\n\n\n\n\n\nGenAI\n\n\nLLMs\n\n\n\nDiscover how to fine-tune a pre-trained GPT-2 model for spam classification in this step-by-step tutorial. Learn data preprocessing techniques, model architecture modification, and the training process using PyTorch. Understand key concepts like freezing layers, cross-entropy loss, and evaluation metrics. See how fine-tuning can significantly improve accuracy from 50% to 95%. Explore ideas for further optimization and adapting the approach to other text classification tasks like sentiment analysis and topic classification.\n\n\n\n\n\nJun 19, 2024\n\n\nHugo Bowne-Anderson and Sebastian Raschka\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping and Training LLMs From Scratch\n\n\n\n\n\n\nGenAI\n\n\nLLMs\n\n\n\nLearn the full lifecycle of building large language models (LLMs) from the ground up. Explore model architecture design, pre-training, fine-tuning, RLHF, and deployment techniques. Discover the skills and hardware needed to work with LLMs, and how to choose between prompt engineering, RAG, and fine-tuning for your use case. Stay up-to-date with cutting-edge research like LoRA, Mixture of Experts, and Direct Preference Optimization. Understand the implications of LLMs for community-driven platforms like Stack Overflow.\n\n\n\n\n\nJun 18, 2024\n\n\nHugo Bowne-Anderson and Sebastian Raschka\n\n\n\n\n\n\n\n\n\n\n\n\n10 Brief Arguments for Local LLMs and AI\n\n\n\n\n\n\nGenAI\n\n\nLLMs\n\n\n\nI recently hosted a session with Simon Willison for the “Mastering LLMs” conference, discussing local LLM usage with Simon’s CLI utility. Reflecting on why I prefer local LLMs, key benefits include data privacy, performance, cost efficiency, customization, offline capabilities, learning opportunities, open-source support, scalability, ethical considerations, and autonomy. Recommended tools for getting started include Ollama, Simon’s LLM CLI utility, LlamaFile, LM Studio, and Oobagooba’s text generation webUI. \n\n\n\n\n\nJun 17, 2024\n\n\nHugo Bowne-Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Generative AI for Everyone\n\n\n\n\n\n\nGenAI\n\n\nLLMs\n\n\n\nWith the advent of Generative AI, more people than ever (technical and non-technical) can build interesting, fun, and productivity-increasing AI apps and workflows. In this post, we introduce the GenAI mindset of combining atomic units to build AI-powered apps and workflows\n\n\n\n\n\nMay 28, 2024\n\n\nHugo Bowne-Anderson and Jonathan Whitaker\n\n\n\n\n\n\n\n\n\n\n\n\nBoost Your Productivity with ChatGPT in 2024: Simple Steps to Get Started\n\n\n\n\n\n\nGenAI\n\n\nLLMs\n\n\n\nHave a 60-page PDF you don’t have time to read and want summarized? A meeting you need action items from? Or an online video you need transcribed and summarized? Would you like them to then generate a PDF, text file, or spreadsheet of the results? You can do all of this with ChatGPT.\n\n\n\n\n\nMay 28, 2024\n\n\nHugo Bowne-Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT, Author of The Quixote\n\n\n\n\n\n\nGenAI\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nMar 24, 2024\n\n\nHugo Bowne-Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nLights, GenAI, Action – Building Systems with Generative Video\n\n\n\n\n\n\nGenAI\n\n\nMetaflow\n\n\n\nWe created a workflow to generate hundreds of videos with Stable Video Diffusion in one command. \n\n\n\n\n\nDec 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Causal Inference?\n\n\n\n\n\n\nCausal inference\n\n\n\nAn Introduction for Data Scientists and Machine Learning Engineers. \n\n\n\n\n\nJul 28, 2022\n\n\n\n\n\n\nNo matching items"
  }
]